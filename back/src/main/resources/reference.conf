
env = "tmp"
env = ${?COMET_ENV}

staging = true
staging = ${?COMET_ARCHIVE}

#spark {
#  yarn.principal = "invalid"
#  yarn.keytab = "invalid"
#  yarn.principal = ${?SPARK_YARN_PRINCIPAL}
#  yarn.keytab = ${?SPARK_YARN_KEYTAB}
#  master = "local[*]"
#  sql.catalogImplementation="hive"
#  sql.catalogImplementation="in-memory"
#}

extraListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker
sql.queryExecutionListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker

// curl -v -H 'Cache-Control: no-cache'  -H 'Content-Type: application/json'  -XPOST localhost:8080/api/experimental/dags/comet_validator/dag_runs -d '{"conf":"{\"key\":\"value\"}"}'

airflow {
  endpoint = "http://127.0.0.1:8080/api/experimental"
  endpoint = ${?AIRFLOW_ENDPOINT}
}


