

spark {
#  yarn.principal = "invalid"
#  yarn.keytab = "invalid"
#  yarn.principal = ${?SPARK_YARN_PRINCIPAL}
#  yarn.keytab = ${?SPARK_YARN_KEYTAB}
  master = "local[*]"
  sql.catalogImplementation="hive"
  #  sql.catalogImplementation="in-memory"
}

extraListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker
sql.queryExecutionListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker

datasets = "/tmp/datasets"
datasets = ${?COMET_DATASETS}

metadata = "/tmp/metadata"
metadata = ${?COMET_METADATA}

launcher=simple
launcher=${?COMET_LAUNCHER}

staging = true
staging = ${?COMET_STAGING}

airflow {
  endpoint = "http://127.0.0.1:8080/api/experimental"
  endpoint = ${?AIRFLOW_ENDPOINT}
}
