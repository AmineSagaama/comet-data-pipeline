

spark {
#  yarn.principal = "invalid"
#  yarn.keytab = "invalid"
#  yarn.principal = ${?SPARK_YARN_PRINCIPAL}
#  yarn.keytab = ${?SPARK_YARN_KEYTAB}
  master = "local[*]"
  sql.catalogImplementation="hive"
  #  sql.catalogImplementation="in-memory"
}

extraListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker
sql.queryExecutionListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker

env = "tmp"
env = ${?COMET_ENV}

staging = true
staging = ${?COMET_ARCHIVE}

airflow {
  endpoint = "http://127.0.0.1:8080/api/experimental"
  endpoint = ${?AIRFLOW_ENDPOINT}

  ingest-command= "/Users/hayssams/programs/spark-2.4.0-bin-hadoop2.7/bin/spark-submit --class com.ebiznext.comet.job.Main /Users/hayssams/git/comet/app/back/target/scala-2.11/comet-assembly-0.1.jar ingest"
  ingest-command = ${?COMET_INGEST_CMD}
}
